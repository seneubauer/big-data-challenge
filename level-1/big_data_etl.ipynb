{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoQ3QlF7w3ZW"
      },
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dh1GJ5rw0WK"
      },
      "outputs": [],
      "source": [
        "# for general use\n",
        "import os\n",
        "\n",
        "# define spark parameters (version info can be found at http://www.apache.org/dist/spark/)\n",
        "spark_version = 'spark-3.2.3'\n",
        "os.environ['SPARK_VERSION'] = spark_version\n",
        "\n",
        "# install spark and java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# set the environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# initialize spark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj0DaeVqw8XM"
      },
      "outputs": [],
      "source": [
        "# install jdbc\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zePEC9TUw76y"
      },
      "source": [
        "## Import Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bTO0g2jw-21"
      },
      "outputs": [],
      "source": [
        "# instantiate the spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmh7I8oWxDd5"
      },
      "outputs": [],
      "source": [
        "# import dependency\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "# locate the buckets\n",
        "url_home_improvement = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\"\n",
        "url_tools = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Tools_v1_00.tsv.gz\"\n",
        "\n",
        "# add the buckets to spark\n",
        "spark.sparkContext.addFile(url_home_improvement)\n",
        "spark.sparkContext.addFile(url_tools)\n",
        "\n",
        "# read spark files into dataframes\n",
        "home_improvement_df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\"), sep = \"\\t\", header = True)\n",
        "tools_df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Tools_v1_00.tsv.gz\"), sep = \"\\t\", header = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TFw4gHHxzEw"
      },
      "source": [
        "## Examine Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NWGx1Cpx0H1"
      },
      "outputs": [],
      "source": [
        "# combine the two data frames\n",
        "df = home_improvement_df.union(tools_df)\n",
        "\n",
        "# store the row counts\n",
        "home_improvement_row_count = home_improvement_df.count()\n",
        "tools_row_count = tools_df.count()\n",
        "row_count = df.count()\n",
        "\n",
        "# check the row counts to ensure a proper union was executed\n",
        "print(f\"{home_improvement_row_count:,} + {tools_row_count:,} = {row_count:,}\")\n",
        "if home_improvement_row_count + tools_row_count == row_count:\n",
        "    print(\"The row counts add up.\")\n",
        "else:\n",
        "    print(\"The row counts don't add up.\")\n",
        "\n",
        "# preview the dataframe\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkl6lYnzx11G"
      },
      "outputs": [],
      "source": [
        "# count null, none, and nan for each of the raw dataframe columns\n",
        "from pyspark.sql.functions import col, isnan, when, count\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hShOJAtox3RK"
      },
      "source": [
        "## Transform Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWtT6s11x482"
      },
      "outputs": [],
      "source": [
        "# clean the raw dataframe and store the results in a new dataframe\n",
        "clean_df = df.dropna().distinct().dropDuplicates([\"review_id\", \"product_id\"])\n",
        "\n",
        "# check how many rows were removed\n",
        "clean_row_count = clean_df.count()\n",
        "print(f\"There are {clean_row_count:,.0f} rows in the clean dataframe. A total of {row_count - clean_row_count:,.0f} rows were removed.\")\n",
        "\n",
        "# confirm the null/none/nan values are gone\n",
        "clean_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in clean_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAH7Oiugx7xM"
      },
      "outputs": [],
      "source": [
        "# check the datatypes for all columns\n",
        "clean_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4MrATAFx8Fx"
      },
      "outputs": [],
      "source": [
        "# convert column data types to match the database schema\n",
        "from pyspark.sql.functions import unix_timestamp, to_date\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# customer_id       str -> int\n",
        "# product_parent    str -> int\n",
        "# star_rating       str -> int\n",
        "# helpful_votes     str -> int\n",
        "# total_votes       str -> int\n",
        "# review_date       str -> date\n",
        "\n",
        "clean_df = clean_df.withColumn(\"customer_id\", clean_df.customer_id.cast(IntegerType()))\n",
        "clean_df = clean_df.withColumn(\"product_parent\", clean_df.product_parent.cast(IntegerType()))\n",
        "clean_df = clean_df.withColumn(\"star_rating\", clean_df.star_rating.cast(IntegerType()))\n",
        "clean_df = clean_df.withColumn(\"helpful_votes\", clean_df.helpful_votes.cast(IntegerType()))\n",
        "clean_df = clean_df.withColumn(\"total_votes\", clean_df.total_votes.cast(IntegerType()))\n",
        "clean_df = clean_df.withColumn(\"review_date\", to_date(unix_timestamp(col(\"review_date\"), \"yyyy-MM-dd\").cast(\"timestamp\")))\n",
        "\n",
        "clean_df.show()\n",
        "clean_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWjzbWrqx_cT"
      },
      "outputs": [],
      "source": [
        "# build the tables\n",
        "review_id_table_df = clean_df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", \"review_date\"]).distinct()\n",
        "products_df = clean_df.select([\"product_id\", \"product_title\"]).distinct()\n",
        "customers_df = clean_df.groupBy([\"customer_id\"]).count().withColumnRenamed(\"count\", \"customer_count\").distinct()\n",
        "vine_table_df = clean_df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\"]).distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZcNBSXyEOc"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nH2M-9zyFo6"
      },
      "outputs": [],
      "source": [
        "# import the confidential Amazon AWS RDS information\n",
        "from config import rds_endpoint, rds_password, rds_dbname, rds_username, rds_port"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhPNY5rjyHUt"
      },
      "outputs": [],
      "source": [
        "# connection string\n",
        "jdbc_url = f\"jdbc:postgresql://{rds_endpoint}:{rds_port}/{rds_dbname}\"\n",
        "\n",
        "# config parameters\n",
        "config = {\n",
        "    \"user\": f\"{rds_username}\",\n",
        "    \"password\": f\"{rds_password}\",\n",
        "    \"driver\": \"org.postgresql.Driver\"\n",
        "}\n",
        "\n",
        "my_mode = \"append\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geL2SSOMyIru"
      },
      "outputs": [],
      "source": [
        "# write to the AWS RDS tables\n",
        "review_id_table_df.write.jdbc(url = jdbc_url, table = \"review_id_table\", mode = my_mode, properties = config)\n",
        "products_df.write.jdbc(url = jdbc_url, table = \"products\", mode = my_mode, properties = config)\n",
        "customers_df.write.jdbc(url = jdbc_url, table = \"customers\", mode = my_mode, properties = config)\n",
        "vine_table_df.write.jdbc(url = jdbc_url, table = \"vine_table\", mode = my_mode, properties = config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD34E3_y8SCl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('PythonData38')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf66be1edf689d8e26383071d71e8172db72bec315ba02c64f9da34706d38082"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
