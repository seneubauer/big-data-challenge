{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mij60Z_HN7n"
      },
      "outputs": [],
      "source": [
        "# for general use\n",
        "import os\n",
        "\n",
        "# define spark parameters (version info can be found at http://www.apache.org/dist/spark/)\n",
        "spark_version = 'spark-3.2.2'\n",
        "os.environ['SPARK_VERSION'] = spark_version\n",
        "\n",
        "# install spark and java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# set the environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# initialize spark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install jdbc\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "metadata": {
        "id": "2d_1k7fI24G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "metadata": {
        "id": "pu8bZj17ljRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in data from S3 bucket\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\"), sep = \"\\t\", header = True)\n",
        "\n",
        "# show the datafarme\n",
        "df.show()"
      ],
      "metadata": {
        "id": "8gS-AZovmLPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of rows in the raw dataframe\n",
        "row_count = df.count()\n",
        "print(f\"There are {row_count} rows in the raw dataframe.\")\n",
        "# There are 2634781 rows in the raw dataframe."
      ],
      "metadata": {
        "id": "D_eyq33TnE4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_id_table_df = df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", \"review_date\"])\n",
        "review_id_table_df.show()"
      ],
      "metadata": {
        "id": "k9C5smZBnZR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_df = df.select([\"product_id\", \"product_title\"])\n",
        "products_df.show()"
      ],
      "metadata": {
        "id": "aR0JCMD8qNsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df = df.groupBy([\"customer_id\"]).count().withColumnRenamed(\"count\", \"customer_count\")\n",
        "customers_df.show()"
      ],
      "metadata": {
        "id": "a8wvOZzwqdfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vine_table_df = df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\"])\n",
        "vine_table_df.show()"
      ],
      "metadata": {
        "id": "mIsVGkiXqlna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rds_endpoint = \"big-data-challenge-db.coojj4dzvalz.us-east-2.rds.amazonaws.com\"\n",
        "rds_password = \"imperator.1991\"\n",
        "rds_dbname = \"reviews_db\"\n",
        "rds_username = \"sneubauer\"\n",
        "rds_port = 5432"
      ],
      "metadata": {
        "id": "62JeEe7xsfl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connection string\n",
        "jdbc_url = f\"jdbc:postgresql://{rds_endpoint}:{rds_port}/{rds_dbname}\"\n",
        "\n",
        "# config parameters\n",
        "config = {\n",
        "    \"user\": f\"{rds_username}\",\n",
        "    \"password\": f\"{rds_password}\",\n",
        "    \"driver\": \"org.postgresql.Driver\"\n",
        "}\n",
        "\n",
        "my_mode = \"overwrite\" # \"append\""
      ],
      "metadata": {
        "id": "gotBvBHg1vOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_id_table_df.write.jdbc(url = jdbc_url, table = \"review_id_table\", mode = my_mode, properties = config)\n",
        "products_df.write.jdbc(url = jdbc_url, table = \"products\", mode = my_mode, properties = config)\n",
        "customers_df.write.jdbc(url = jdbc_url, table = \"customers\", mode = my_mode, properties = config)\n",
        "vine_table_df.write.jdbc(url = jdbc_url, table = \"vine_table\", mode = my_mode, properties = config)"
      ],
      "metadata": {
        "id": "1AkDS3iX2LdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}